\section{Evaluation}
\label{sec:evaluation}
We conduct extensive evaluation to study the attack detection effectiveness of \sysnameF (\S\ref{sub:evaluation-detection}), and the performance of \prototype in synthetic (\S\ref{subsub:syn}) and real-world (\S\ref{subsub:real}) workloads. We summarize the main results as follows:
\begin{itemize}[leftmargin=*]
\item \sysnameF effectively finds similar plaintext chunks based on the corresponding ciphertext chunks. For example, it  detects up to 80.2\% similar chunks when they have medium modifications from each other.
\item \sysnameF effectively detects the learning-content attack even if the adversary submits enumerated contents within ordinary content snapshots. Meanwhile, it introduces low misjudgements (e.g., zero in the default configuration) for real-world data.
\item \prototype incurs limited upload (e.g., 8.8\%) and download (e.g., 0.8\%) performance overhead over SGXDedup (i.e., without \sysnameF) when processing large-scale real-world data.
\item Compared to existing approaches \cite{harnik10, li15} that are secure against the learning-content attack, \prototype is bandwidth efficient (e.g., saving the network traffic by up to 98.9\%).
\end{itemize}

\subsection{Datasets}
\label{sub:datasets}


\paragraph{Synthetic datasets.}
We consider three types of synthetic datasets for our evaluation. The first datasets are generated by modifying a base chunk in a controlled manner. Specifically, we first create an 8\,KiB base chunk $M$.  Each time, we randomly choose $x$ locations in $M$, and change $y$ bytes in each location, in order to create a modified chunk. After 10\,K modifications, we obtain a synthetic dataset {\em SYNChunk}$(x, y)$, which includes 10\,K chunks that are modified from the same base chunk $M$. We use SYNChunk$(x, y)$ as the ground truth (of similar chunks) to study the effectiveness of similarity detection (Exp\#1 and Exp\#2).

In addition, we consider {\em SYNFile}$(x, y)$ as a file that has a size of 4\,KiB. We assume that SYNFile$(x, y)$ includes $x$ unknown variables (whose values are to be inferred) in the file content, and the value of each variable is chosen from a space of size $y$. We use SYNFile$(x, y)$ as the target file to study the detection effectiveness of \sysnameF in different cases (Exp\#4). We do not consider larger target files, since the adversary needs to enumerate more similar chunks (for attack) and is more likely to be caught by \sysnameF.

Furthermore, we generate {\em SYNUnique} as a set of 2\,GiB files, comprising globally non-duplicate chunks each. We use SYNUnique to perform stress test on the performance of \prototype (\S\ref{subsub:syn}).



\paragraph{Real-world datasets.} We consider four real-world datasets, whose characteristics are summarized in Table~\ref{tab:datasets}: (i) {\em FSL} \cite{fsl}, which includes 795 home directory snapshots of nine students from January 22 to June 17 in 2013; (ii) {\em MS} \cite{meyer11}, which includes 143 windows file system snapshots, each having a logical size of about 100\,GiB; (iii) {\em Linux} \cite{linux}, which includes 84 snapshots from the stable versions (between v2.6.11 and v5.13) of Linux source code; (iv) {\em CouchDB} \cite{couchdb}, which includes 83 docker images of CouchDB in the versions between v2.5.2 and v6.6.2 under general, community, and enterprise distributions.
Note that FSL and MS only include chunk metadata (e.g., fingerprints and sizes) rather than actual data; we will replay both traces to evaluate the performance of \prototype in processing large-scale workloads (Exp\#9).


\begin{table}
  \centering
  \small
  \begin{tabular}{|l|l|l|l|}
    \hline
    {\bf Datasets} & {\bf Snapshots} & {\bf Raw Size} & {\bf Deduplication Ratio} \\
    \hline
    \hline
    FSL & 795 & 56.2\,TiB & 140.4 \\
    \hline
    MS & 143 & 14.4\,TiB & 6.0 \\
    \hline
    Linux & 84 & 44.9\,GiB & 1.3 \\
    \hline
    CouchDB & 83 & 22.9\,GiB & 1.5 \\
    \hline
  \end{tabular}
  \caption{Characteristics of real-world datasets. The deduplication ratio is defined as the rate of the size of pre-deduplicated data by that of the data after deduplication. A higher deduplication ratio means that the corresponding dataset includes more redundancies.}
  \vspace{-6pt}
  \label{tab:datasets}
\end{table}




  % from the File systems and Storage Lab (FSL) at Stony Brook University. The dataset contains a list of 48-bit fingerprints of all chunks in each snapshot, and the average chunk size is 8 KB. We use all snapshots from January 22 to June 17 in 2013, containing a total of 56.2 TiB pre-deduplication data. The size of the dataset was reduced to 431.9 GiB after deduplication.
  %    {\em MS} includes windows file system snapshots from Microsoft. Each snapshot contains a list of 40-bit fingerprints of all chunks (average size of 8 KB). We selected 143 snapshots with a logical size of about 100GB from 857 original snapshots. Our dataset contains 14.4 TiB of pre-deduplicated data, and the data volume after deduplication is 2.4 TiB.
  %    {\em Linux Kernel} contains the source code of each official version of the Linux kernel. Specifically, we obtained historical modifications from Github and exported non-RC or beta releases, which contains totaling 84 versions from 2.6.11 to 5.13. It contains a total of 44.94 GiB of pre-deduplicated data, which reduces to 33.41 GiB after deduplication.
  %    {\em Docker images}\cite{couchDB} is generated based on the Docker images of each CouchDB version from DockerHub. CouchDB is a widely used database that uses JSON for documents, an HTTP API, \& JavaScript/declarative indexing. The dataset includes three distribution channels of general, community, and enterprise, with a total of 83 versions of a complete image from version 2.5.2 to version 6.6.2. Before deduplication, its size is 22.88 GiB, while after deduplication, it becomes 14.95 GiB.
