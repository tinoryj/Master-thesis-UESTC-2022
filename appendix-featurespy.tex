\section{\sysnameF Simulator}

The simulator is used to run the experiments in §6.2 in our paper. For brevity, we only focus on the simulation of the overall FeatureSpy (i.e., Exp\#3 and Exp\#4).

\subsection{Dependencies}

FeatureSpy simulator is developed under Ubuntu 20.04.3 LTS and depends on the following packages that need to be installed manually or by the default `apt` package management tool.
\begin{enumerate}[leftmargin=*]
    \item OpenSSL version 1.1.1l [Donwload Link](https://www.openssl.org/source/old/1.1.1/openssl-1.1.1l.tar.gz)
    \item libssl-dev (For FeatureSpy encryption algorithm)
    \item clang/llvm (For compiling source code)
    \item python (For faking offer file generator)
    \item git (System components used for downloading Linux datasets)
    \item curl/golang/jq (System components used for downloading CouchDB datasets)
\end{enumerate}


The above packages can be installed via the `apt` package management tool with the following commands on Ubuntu.

\begin{lstlisting}[style=shell]
sudo apt install -y build-essential openssl libssl-dev clang llvm python curl git golang jq
\end{lstlisting}

\subsection{Build}

Compile and cleanup the FeatureSpy simulator as follows.

\begin{lstlisting}[style=shell]
cd FeatureSpy
# compile
make
# cleanup
make clean
\end{lstlisting}

\subsection{Usage}

\subsubsection{Manual}

**Step 1:** Download the Linux/CouchDB datasets. The downloaded Linux and CouchDB snapshots will be saved in `FeatureSpy/traceDownload/linuxTrace` and `FeatureSpy/traceDownload/couchTrace/packed\_enterprises`, respectively.

\begin{lstlisting}[style=shell]
cd FeatureSpy/traceDownload
chmod +x *.sh
bash downloadTraceCouch.sh
bash downloadTraceLinux.sh
\end{lstlisting}

**Step 2:** Generate faked offers that enumerate salary and sign-on bonus.  We have packaged the original Google's offer letter in `FeatureSpy/SimulateOfferGenerator`. Alternatively, it is feasible to download the original offer from [here](https://www.sec.gov/Archives/edgar/data/1288776/000119312508140342/dex101.htm). In addition to the default range in our paper, you can modify the parameters in `generateFakeOffers.sh` to change the ranges and cardinalities of the salary and sign-on bonus, respectively. The generated offers are stored in the `FeatureSpy/SimulateOfferGenerator/result` directory.

\begin{lstlisting}[style=shell]
cd FeatureSpy/SimulateOfferGenerator
chmod +x generateFakeOffers.sh
bash generateFakeOffers.sh
\end{lstlisting}

**Step 3:** Generate the list of the faked offer files.

\begin{lstlisting}[style=shell]
cd FeatureSpy
chmod +x *.sh
# param 1: path to the folder where the target files are stored
# param 2: path to store the generated file list
bash generateFileList.sh SimulateOfferGenerator/result/ offer.fileList
\end{lstlisting}

An example of the file list is shown below. Each line corresponds to an adversarially faked file that enumerates the possible values of the salary and sign-on bonus.

\begin{lstlisting}[style=c++]
/home/xxx/xxx/FeatureSpy/SimulateOfferGenerator/result/offer-base-204-30.html
/home/xxx/xxx/FeatureSpy/SimulateOfferGenerator/result/offer-base-204-31.html
/home/xxx/xxx/FeatureSpy/SimulateOfferGenerator/result/offer-base-204-32.html
/home/xxx/xxx/FeatureSpy/SimulateOfferGenerator/result/offer-base-204-33.html
/home/xxx/xxx/FeatureSpy/SimulateOfferGenerator/result/offer-base-204-34.html
...
\end{lstlisting}

**Step 4:** Generate the list of files in each Linux/CouchDB snapshot. Here we use Linux v5.13 as an example, and store the untared contents in in `FeatureSpy/v5.13`.

\begin{lstlisting}[style=shell]
cd FeatureSpy
chmod +x *.sh
tar -xvf traceDownload/linuxTrace/v5.13.tar -C v5.13
# param 1: path to the folder where the target files are stored
# param 2: path to store the generated file list
bash generateFileList.sh v5.13 v5.13.fileList
\end{lstlisting}

**Step 5:** Randomly insert the faked offers into the target snapshot to form an attack snapshot, perform chunking on each individual file of the attack snapshot and output the sequential list of chunk metadata of the target snapshot.

\begin{lstlisting}[style=shell]
cd FeatureSpy
make clean
make
# param 1: file list of inserted snapshots (generated by step 4); use v5.13.fileList as an example
# param 2: file list of fake offers (generated by step 3); use result.fileList as an example
# param 3: Redirect chunking's stdout to the specified file for recording chunk information; use test.chunkInfo as an example
./chunking v5.13.fileList result.fileList > test.chunkInfo
\end{lstlisting}

An example of the list of chunk metadata (`test.chunkInfo`) is shown below. The metadata contains the sequence number, hash, and size of each chunk.

\begin{lstlisting}[style=c++]
0
C15D6659778FD34B4F62C8C069E0FCC86265D118B178BD4D11002C2F1DF22316
8894
1
ECD6F5A896E0900A368BDCCFFA8CA9BD94BFD50278A7025EB857A8F96487B7B6
7268
2
F438756CB77F09C31FBD80A36B3454BE34054F4403E1BABF5D8201D1A77AA553
2799
...
\end{lstlisting}

**Step 6:** Perform similarity-preserving encryption on each chunk, and detect the learning-content attack based on ciphertext chunks.

\begin{lstlisting}[style=shell]
# param 1: Chunk info list generated by chunking; use test.chunkInfo as an example
# param 2: Window size (W); use the default  window size W=5000 as an example
# param 3: Indicator length (L) unit:block(16byte); use the default indicator length L=2 as an example
# param 4: detection threshold (T) unit:%; use the default threshold T=0.03 as an example
# param 5: Redirect FeatureSpy's stdout to the specified file for recording the maximum frequency of each window under the three schemes and duplicate statistics; use result-mixed.csv as an example
./FeatureSpy test.chunkInfo 5000 2 0.03 > result-mixed.csv
\end{lstlisting}

The output (in `stderr`) of `FeatureSpy` shows whether each scheme detects the learning-content attack (e.g., the `minFeature` scheme detected the attack, while `firstFeature` and `allFeature` schemes did not).

\begin{lstlisting}[style=shell]
firstFeature: not detected
minFeature: detected
allFeature: not detected
\end{lstlisting}

The format of the `result-mixed.csv` file is as follows. Each row represents the fraction of the most number of chunks that have the same similarity indicator in a window.

| firstFeature | minFeature | allFeature |
| ------------ | ---------- | ---------- |
| 0.08         | 0.08       | 0.08       |
| 0.001        | 0.002      | 0.001      |
| 0.003        | 0.003      | 0.003      |

**Step 7 (optional):** If you want to test whether `FeatureSpy`  produces false positives on raw snapshots without inserting faked offers, you need to skip the second parameter of `chunking` (Step 5) and only perform chunking on the file list of the target snapshot. Then, run  `FeatureSpy` to get the result as in Step 6.


\begin{lstlisting}[style=shell]
# param 1: file list of inserted snapshots (generated by step 4); use v5.13.fileList as an example
# param 2: Redirect chunking's stdout to the specified file for recording chunk information; use test.chunkInfo as an example
./chunking v5.13.fileList > test.chunkInfo
./FeatureSpy test.chunkInfo 5000 2 0.03 > result-raw.csv
\end{lstlisting}

\subsubsection{Automatic scripts}

Alternatively, we provide a quick way to analyze the detection effectiveness. You can use `runCouch.sh` and `runLinux.sh` to test FeatureSpy with CouchDB and Linux, respectively, and use `processResult.sh` to output the summary of results. Note that you can modify the parameters in `runCouch.sh` and `runLinux.sh` to test different window sizes and similarity indicator lengths.

\begin{lstlisting}[style=shell]
# download trace
cd FeatureSpy/traceDownload
chmod +x *.sh
bash downloadTraceCouch.sh
bash downloadTraceLinux.sh
# generate fake offers
cd FeatureSpy/SimulateOfferGenerator
chmod +x generateFakeOffers.sh
bash generateFakeOffers.sh
# test with CouchDB and Linux trace
cd FeatureSpy/
chmod +x *.sh
bash runCouch.sh
bash runLinux.sh
bash processResult.sh
\end{lstlisting}

In the running of `runLinux.sh`/`runCouch.sh`, the program processes each snapshot and outputs the detection results in the command line (via `stderr`).

\begin{lstlisting}[style=shell]
firstFeature: not detected
minFeature: detected
allFeature: not detected
\end{lstlisting}

Also, it saves the feature distribution information of each window of the processing snapshot in `FeatureSpy/linuxResult/` and  `FeatureSpy/couchResult/` for Linux and CouchDB dataset, respectively. The file name of each raw snapshot is `${snpashotID}-origin-${windowSize}-${indicatorLength}.csv`, while that of each attack snapshot (i.e., adversarially injected with fake offers) is `${snpashotID}-mixed-${windowSize}-${indicatorLength}.csv`. An example file is shown as follows, where each row represents the fraction of the most number of chunks that have the same similarity indicator in a window.

| firstFeature | minFeature | allFeature |
| ------------ | ---------- | ---------- |
| 0.08         | 0.08       | 0.08       |
| 0.001        | 0.002      | 0.001      |
| 0.003        | 0.003      | 0.003      |

Furthermore, the script `processResult.sh` generates the file `mergedLinuxResult-${windowSize}-${indicatorLength}.csv`/`mergedCouchResult-${windowSize}-${indicatorLength}.csv` to summarize the final results of attack detection for all snapshots (see below). Each row represents the maximum fraction of the most number of chunks that have the same similarity indicator among all windows in each snapshot. Note that the raw (mixed) here means the snapshot without (with) injected faked offers.

| firstFeature max freq (raw) | minFeature max freq (raw) | allFeature max freq (raw) | firstFeature max freq (mixed) | minFeature max freq (mixed) | allFeature max freq (mixed) |
| :-------------------------- | ------------------------- | ------------------------- | ----------------------------- | --------------------------- | --------------------------- |
| 0.0032                      | 0.0028                    | 0.0028                    | 0.1106                        | 0.1106                      | 0.1106                      |
| 0.003                       | 0.003                     | 0.003                     | 0.1022                        | 0.1022                      | 0.1022                      |
| 0.0026                      | 0.0024                    | 0.0024                    | 0.1026                        | 0.1026                      | 0.1026                      |
| 0.0026                      | 0.0022                    | 0.0022                    | 0.0972                        | 0.0972                      | 0.0972                      |


\section{\prototype}

The system SGXDedup+ augments the previous SGX-based encrypted deduplication system [SGXDedup](https://www.usenix.org/conference/atc21/presentation/ren-yanjing) with FeatureSpy, so as to detect the learning-content attack in a client-side trust-execution environment.


\subsection{Prerequisites}

SGXDedup+ is tested on a machine that equips with a Gigabyte B460M-DS3H motherboard and an Intel i7-10700 CPU, and runs Ubuntu 20.04.3 LTS.

Before running SGXDedup+, check if your machine supports SGX. If there is an option as `SGX` or `Intel Software Guard Extensions` in BIOS, then enable the option; otherwise, your machine does not support SGX. We strongly recommend finding the SGX-supported device in the [SGX hardware list](https://github.com/ayeks/SGX-hardware).

\subsection{Registration}

SGXDedup+ uses EPID-based remote attestation, and you need to register at the [EPID attestation page](https://api.portal.trustedservices.intel.com/EPID-attestation). Then, you can find your SPID and the corresponding subscription keys (both the primary and the secondary keys) at the [products page](https://api.portal.trustedservices.intel.com/products). Our test uses the `DEV Intel® Software Guard Extensions Attestation Service (Unlinkable)` product.


\subsection{Dependencies}

SGXDedup+ depends on the following packages that need to be installed manually or by package management tools.
\begin{enumerate}[leftmargin=*]
    \item Intel® Software Guard Extensions (Intel® SGX) driver version 2.11.0\_2d2b795 [Download Link](https://download.01.org/intel-sgx/sgx-linux/2.15.1/distro/ubuntu20.04-server/sgx\_linux\_x64\_driver\_2.11.0\_2d2b795.bin)
    \item Intel® SGX SDK version 2.15.101.1 [Download Link](https://download.01.org/intel-sgx/sgx-linux/2.15.1/distro/ubuntu20.04-server/sgx\_linux\_x64\_sdk\_2.15.101.1.bin)
    \item Intel® SGX SSL version lin\_2.15.1\_1.1.1l [Download Link](https://github.com/intel/intel-sgx-ssl/archive/refs/tags/lin\_2.15.1\_1.1.1l.zip)
    \item OpenSSL version 1.1.1l [Donwload Link](https://www.openssl.org/source/old/1.1.1/openssl-1.1.1l.tar.gz)
    \item SGX packages including `libsgx-epid` `libsgx-quote-ex` `libsgx-dcap-ql` `ibsgx-urts` `libsgx-launch` `libsgx-enclave-common-dev` `libsgx-uae-service`... etc (See SGX Installation Guide [Download Link](https://download.01.org/intel-sgx/sgx-linux/2.15.1/docs/Intel\_SGX\_SW\_Installation\_Guide\_for\_Linux.pdf) for detail).
    \item libssl-dev (For SGXDedup+ encryption algorithm)
    \item libboost-all-dev (For SGXDedup+ multithreading, message transmission, etc.)
    \item libleveldb-dev (FOr Sgxdedup+ Deduplication Index Based On Leveldb)
    \item libsnappy-dev (Required by LevelDB)
    \item build-essential (Basic program compilation environment)
    \item cmake (CMake automated build framework)
    \item wget (System components used for remote attestation requests)
\end{enumerate}

We now provide a one-step script to automatically install and configure the dependencies. The script will ask for a password for sudo operations if necessary. We have tested the script on Ubuntu 20.04.3 LTS.

\begin{lstlisting}[style=shell]
chmod +x Scripts/environmentInstall.sh
./Scripts/environmentInstall.sh
\end{lstlisting}

Restart is required after the installation is finished. Then, check whether both `sgx\_enclave` and `sgx\_provision` are in `/dev`. If they are not in the directory (i.e., SGX driver is not successfully installed), reinstall the SGX driver manually and restart the machine until `sgx\_enclave` and `sgx\_provision` are in `/dev`. We strongly recommend that you refer to the instructions of [SGX Installation Guide: Download Link](https://download.01.org/intel-sgx/sgx-linux/2.15.1/docs/Intel\_SGX\_SW\_Installation\_Guide\_for\_Linux.pdf) and [SGX SSL README: Link](https://github.com/intel/intel-sgx-ssl) during manual or automatic installation for troubleshooting.


\subsection{Configuration}

SGXDedup+ is configured based on JSON. You can change its configuration without rebuilding. We show the default configuration (`./config.json`) of SGXDedup+ as follows.

\begin{lstlisting}[style=json]
{
    "ChunkerConfig": {
        "_chunkingType": "VariableSize", // FixedSize: fixed size chunking; VariableSize: variable size chunking; TraceFSL: FSL dataset hash list; TraceMS: MS dataset hash list
        "_minChunkSize": 4096, // The smallest chunk size in variable size chunking, Uint: Byte (Maximum size 16KiB)
        "_avgChunkSize": 8192, // The average chunk size in variable size chunking and chunk size in fixed size chunking, Uint: Byte (Maximum size 16KiB)
        "_maxChunkSize": 16384, // The biggest chunk size in variable size chunking, Uint: Byte (Maximum size 16KiB)
        "_slidingWinSize": 48, // The sliding window size for rabin fingerprinting in variable size chunking, Uint: Byte
        "_ReadSize": 256 // System read input file size every I/O operation, Uint: MiB
    },
    "KeyServerConfig": {
        "_keyBatchSize": 4096, // Maximum number of keys obtained per communication
        "_keyEnclaveThreadNumber": 1, // Maximum thread number for key enclave
        "_keyServerRArequestPort": 1559, // Key server host port for receive key enclave remote attestation request
        "_keyServerIP": [
            "127.0.0.1"
        ], // Key server host IP ()
        "_keyServerPort": [
            6666
        ], // Key server host port for client key generation
        "_keyRegressionMaxTimes": 1048576, // Key regression maximum numbers `n`
        "_keyRegressionIntervals": 25920000 // Time interval for key regression (Unit: seconds), used for key enclave. Should be consistent with "server.\_keyRegressionIntervals"
    },
    "SPConfig": {
        "_storageServerIScriptsP": [
            "127.0.0.1"
        ], // Storage server host IP
        "_storageServerPort": [
            6668
        ], // Storage server host port for client upload or download files
        "_maxContainerSize": 8388608 // Maximum space for one-time persistent chunk storage, Uint: Byte (Maximum size 8MiB)
    },
    "pow": {
        "_quoteType": 0, // Enclave quote type, do not modify it
        "_iasVersion": 3, // Enclave IAS version, do not modify it
        "_iasServerType": 0, // Server IAS version, do not modify it
        "_batchSize": 4096, // POW enclave batch size (Unit: chunks)
        "_ServerPort": 6669, // The port on storage server for remote attestation
        "_enclave\_name": "pow\_enclave.signed.so", // The enclave library name to create the target enclave
        "_SPID": "", // Your SPID for remote attseation service
        "_PriSubscriptionKey": "", // Your Intel remote attestation service primary subscription key
        "_SecSubscriptionKey": "" // Your Intel remote attestation service secondary subscription key
    },
    "km": {
        "_quoteType": 0, // Enclave quote type, do not modify it
        "_iasVersion": 3, // Enclave IAS version, do not modify it
        "_iasServerType": 0, // Server IAS version, do not modify it
        "_ServerPort": 6676, // The port on storage server for remote attestation
        "_enclave\_name": "km\_enclave.signed.so", // The enclave library name to create the target enclave
        "_SPID": "", // Your SPID for remote attseation service
        "_PriSubscriptionKey": "", // Your Intel remote attestation service primary subscription key
        "_SecSubscriptionKey": "" // Your Intel remote attestation service secondary subscription key
    },
    "server": {
        "_RecipeRootPath": "Recipes/", // Path to the file recipe storage directory
        "_containerRootPath": "Containers/", // Path to the unique chunk storage directory
        "_fp2ChunkDBName": "db1", // Path to the chunk database directory
        "_fp2MetaDBame": "db2" // Path to the file recipe database directory
        "_raSessionKeylifeSpan": 259200000 // Time interval for key regression (Unit: seconds), used for storage server. Should be consistent with "KeyServerConfig.\_keyRegressionIntervals"
    },
    "client": {
        "_clientID": 1, // Current client ID
        "_sendChunkBatchSize": 1000, // Maximum number of chunks sent per communication
        "_sendRecipeBatchSize": 100000, // Maximum number of file recipe entry sent per communication
        "_keySeedFeatureNumber": 12, // Total number of sub-features generated in FBE. Should be set to 4 when using firstFeature, and 12 when using min/allFeature
        "_keySeedSuperFeatureNumber": 3, // Total number of features generated in FBE (1/4 of \_keySeedFeatureNumber by default). Should be set to 1 when using firstFeature, and 12 when using min/allFeature
        "_keySeedGenFeatureThreadNumber": 3 // Thread number of FBE, default to 3
    }
}
\end{lstlisting}

Before starting, you need to fill the SPID and subscription keys in `./config.json` based on your registration information in Intel.

\begin{lstlisting}[style=json]
...
"pow": {
    ...
    "_SPID": "", // Your SPID for remote attseation service
    "_PriSubscriptionKey": "", // Your Intel remote attestation service primary subscription key
    "_SecSubscriptionKey": "" // Your Intel remote attestation service secondary subscription key
},
"km": {
    ...
    "_SPID": "", // Your SPID for remote attseation service
    "_PriSubscriptionKey": "", // Your Intel remote attestation service primary subscription key
    "_SecSubscriptionKey": "" // Your Intel remote attestation service secondary subscription key
},
...
\end{lstlisting}

You can modify `include/systemSettings.hpp` to adjust FeatureSpy's parameters similarity indicator length (L), window size (W), threshold (T), as well as the underlying feature extraction schemes `firstFeature`, `minFeature`, and `allFeature`.


\begin{lstlisting}[style=c++]
...
/* FBE Key Generation method Settings */
#define SUPER\_FEATURE\_GEN\_METHOD FIRST\_FEATURE
// Alternative schemes: MIN\_FEATURE, ALL\_FEATURE

/* FBE Key Generation method Settings */
#define PREFIX\_LENGTH 32 // Two encryption blocks by default
#define PREFIX\_FREQUENCY\_THRESHOLD 15 // Corresponding to T*W in the paper
#define COUNT\_WINDOW\_SIZE 5000 // Default window size W = 5000
...
\end{lstlisting}

\subsection{Build}

Compile SGXDedup+ as follows.

\begin{lstlisting}[style=shell]
mkdir -p bin && mkdir -p build && mkdir -p lib && cd build
cmake -DCMAKE\_BUILD\_TYPE=Release .. && make

cd ..
cp lib/*.a bin/
cp ./lib/pow\_enclave.signed.so ./bin
cp ./lib/km\_enclave.signed.so ./bin
cp config.json bin/
cp -r key/ bin/
mkdir -p bin/Containers && mkdir -p bin/Recipes
\end{lstlisting}

Alternatively, we provide a script for a quick build and clean-up, and you can use it.

\begin{lstlisting}[style=shell]
chmod +x ./Scripts/*.sh
# Build SGXDedup+ in release mode
./Scripts/buildReleaseMode.sh
# Build SGXDedup+ in debug mode
./Scripts/buildDebugMode.sh
# Clean up build result
./Scripts/cleanBuild.sh
\end{lstlisting}

The generated executable file and its required enclave dynamic library, keys are all stored in the `bin` directory.

\subsection{Usage}

You can test SGXDedup+ in a single machine, and connect the key server, cloud, and client instances via the local loopback interface in `bin` directory. Since the key enclave needs to be attested by the cloud before usage, you need to start the cloud (`server-sgx`) first, then start the key server (`keymanager-sgx`), and wait for the message `KeyServer : keyServer session key update done` that indicates a successful attestation.

\begin{lstlisting}[style=shell]
# start cloud
cd bin
./server-sgx

# start key server
cd bin
./keymanager-sgx
\end{lstlisting}

SGXDedup+ provides store and restores interfaces to clients.

\begin{lstlisting}[style=shell]
# store file
cd bin
./client-sgx -s file

# restore file
cd bin
./client-sgx -r file
\end{lstlisting}
